# ğŸ–¼ï¸ BLIP-Based Image Captioning System

A **Vision-Language Model (VLM)** powered by **BLIP (Bootstrapped Language-Image Pretraining)** for generating concise and detailed captions from images and their associated metadata.

---

## ğŸš€ Why BLIP?

- âœ… **Open-source** and well-maintained  
- ğŸ† **State-of-the-art performance** on image-text understanding tasks  
- ğŸ’¬ **Conditional generation**: Uses both image and text prompts  
- ğŸ”§ Easily **fine-tunable** on custom datasets  

---

## ğŸ§  Model Architecture

- **Vision Encoder**: ViT (Vision Transformer) â€“ for image embeddings  
- **Text Decoder**: Transformer-based language model  
- **Multimodal Fusion**: Cross-attention between image & text features  

---

## ğŸ—‚ï¸ Data Preprocessing

### ğŸ”¹ Input Data Format

- **Images**: `.jpg`, `.png` (can be tables, graphs, diagrams, etc.)  
- **Metadata**: Structured `.txt` or `.json` files per image  

### ğŸ”§ Preprocessing Steps

#### ğŸ–¼ Image Processing

- Resize to **512Ã—512** (ViT-compatible)  
- Convert to **RGB**  
- Normalize pixel values (using standard mean/std)  

#### ğŸ“ Text Processing

- **Context Concatenation**: Combine metadata fields into structured prompts  
- **Truncation**: Limit to **200 tokens** (BLIP max input length)  

---

## ğŸ‹ï¸ Fine-Tuning Methodology

### ğŸ“ Dataset Preparation

- Custom `Dataset` class to load:  
  - Image  
  - Metadata (context prompt)  
  - Target caption  
- Train/Validation Split: **80% / 20%**

### âš™ï¸ Training Configuration

- **Framework**: ğŸ¤— Hugging Face `Trainer`  
- **Hyperparameters**:  
  - Batch Size: `8`  
  - Epochs: `3`  
  - Learning Rate: `5e-5`  
  - Optimizer: `AdamW`  
  - Loss: `Cross-Entropy`  

---

## ğŸ“Š Evaluation Metrics

| Metric       | Focus                                   |
|--------------|------------------------------------------|
| **BLEU**     | N-gram precision                         |
| **ROUGE-L**  | Longest common subsequence (recall)      |
| **BERTScore**| Semantic similarity                      |

---

## ğŸ§¾ Caption Generation Pipeline

### ğŸ”„ Step-by-Step Inference

1. **Load** image and corresponding metadata  
2. **Generate Captions**:  
   - Short Caption â†’ `max_length = 20`  
   - Detailed Caption â†’ `max_length = 64`  
3. **Confidence Scoring**:  
   - Based on: `exp(mean(token logits))`  
   - Low confidence (< 0.5) â†’ ğŸ”¶ *highlighted in orange*  
4. **Consistency Check**:  
   - Validate that captions do not contradict metadata (e.g., section headers, keywords)  

---

