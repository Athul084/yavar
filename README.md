# Image Captioning from Contextual Metadata using BLIP

A fine-tuned Vision-Language Model (VLM) that generates context-aware captions for images using visual content and surrounding metadata.

![Example Output](docs/sample_output.jpg) *(example annotated image with captions)*

## Features

- ğŸ–¼ï¸ **Multimodal Understanding**: Combines image features with textual metadata
- ğŸ“ **Dual Captioning**: Generates both concise and detailed descriptions
- ğŸ” **Context-Aware**: Leverages section headers, captions, footnotes, etc.
- ğŸ“Š **Confidence Scoring**: Quantifies prediction certainty for each caption
- ğŸ› ï¸ **Fine-Tuning**: Adapts BLIP to domain-specific data

## Model Architecture

### Base Model: BLIP (Bootstrapped Language-Image Pre-training)
**Why BLIP?**
- âœ… Open-source (Apache 2.0 license)
- âœ… State-of-the-art on image-text tasks
- âœ… Supports conditional generation
- âœ… Fine-tunable on custom datasets

**Components**:
| Component | Description |
|-----------|-------------|
| Vision Encoder | ViT (Vision Transformer) for image embeddings |
| Text Decoder | Transformer-based language model |
| Multimodal Fusion | Cross-attention between image & text features |

## Data Processing

### Input Structure
